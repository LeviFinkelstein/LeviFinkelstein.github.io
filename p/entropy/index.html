<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Levi Finkelstein" />
  <title>Information Entropy</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      word-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <link rel="stylesheet" href="../../index.css">
</head>
<body>
<header id="title-block-header">
<h1 class="title">Information Entropy</h1>
<p class="author">Levi Finkelstein</p>
<p class="date">04 09 2021</p>
</header>
<h4 id="i.">I.</h4>
<p>Imagine that you’re a detective faced with a murder and a list of <span class="math inline">\(16\)</span> suspects each whom is equally likely to have done it prior to you starting your investigation.</p>
<p>When you find a clue indicating that the culprit is a woman you can remove the <span class="math inline">\(8\)</span> male suspects, leaving you with <span class="math inline">\(8\)</span> female suspects. How do you quantify how much information this clue gave you? One way is to specify the fraction by which it scaled the search space, which in this case was by a half: <span class="math display">\[\frac{1}{2}*16 = 8\]</span> So <span class="math inline">\(\frac{1}{2}\)</span> was the information gain. This means you need <span class="math inline">\(\frac{1}{16}\)</span> information in total to find the murderer, i.e. to reduce the search space from <span class="math inline">\(16\)</span> to <span class="math inline">\(1\)</span>.</p>
<p>Another way to specify this is to talk about how many times you require the search space be halved. In this case: <span class="math display">\[(\frac{1}{2})^N*16 = 1 \Rightarrow N=4\]</span> So you need <span class="math inline">\(4\)</span> halvings.</p>
<p>An equivalent way of saying this is that you need <span class="math inline">\(4\)</span> bits. The reason it makes sense to equate bits to halvings is that a bit can only take on two values, so each of <span class="math inline">\(1\)</span> and <span class="math inline">\(0\)</span> would exclude each their own equally sized disjoint sections of the search space (for example <span class="math inline">\(0=men\)</span> and <span class="math inline">\(1=women\)</span>). <strong>If you learn 1 bit of information you reduce the search space by a half.</strong></p>
<p>In general if there are <span class="math inline">\(S\)</span> suspects you need clues worth a total of <span class="math inline">\(N=\log_2{S}\)</span> bits of information since <span class="math display">\[(\frac{1}{2})^N*S=1 \Rightarrow (\frac{1}{2})^N=\frac{1}{S}\Rightarrow 2^N=S\]</span> <span class="math display">\[\Rightarrow \log_2{2^N}=\log_2{S} \Rightarrow N = \log_2{S}\]</span></p>
<p>Conversely, since <span class="math inline">\(2^N = S\)</span> you can also think of <span class="math inline">\(N\)</span> (the number of bits) as how many doublings you need to go from <span class="math inline">\(1\)</span> to the size of the initial search space <span class="math inline">\(S\)</span>.</p>
<h4 id="ii.">II.</h4>
<p>In this case the <em>probability</em> of a suspect being the murderer is equal to the fraction they take up in the current search space. So before any clues were discovered each on of them had a probability <span class="math inline">\(p=\frac{1}{16}\)</span> of being guilty. After discovering the first clue the men were excluded from the search space, receiving no fraction of it (<span class="math inline">\(p=0\)</span>), with the women receiving a bigger fraction <span class="math inline">\(p=\frac{1}{8}\)</span> of the remaining space.</p>
<p>From this you can see that <strong>the more information you gain the more probability mass gets concentrated in a smaller space</strong>. When all the probability mass gets concentrated at a single point (<span class="math inline">\(p=1\)</span> that a single person is the murderer) you’ve gained all the information you can.</p>
<p>In a converse way, the more information there is (to gain?) the more probability mass is spread out. This way to think about information involves considering how many times (how many bits) you need to take half the probability mass and concentrate it on another half until you end up with everything on single point.</p>
<p>Before finding the first clue the probability of the guilty being a woman was <span class="math inline">\(p=\frac{1}{2}\)</span>. So encountering an event that <em>used</em> to be <span class="math inline">\(p=\frac{1}{2}\)</span> means now setting it to <span class="math inline">\(p=1\)</span> by stealing all the probability mass that’s outside the event, in this case the <span class="math inline">\(\frac{1}{2}\)</span> of mass contained in the "the murderer is a man" event.</p>
<p>The number of bits gained by learning a probability <span class="math inline">\(p\)</span> fact is then equal to <span class="math display">\[\log_2{\frac{1}{p}}=\log_2{p^{-1}}=-\log_2{p}\]</span> Because:</p>
<ol>
<li><p><span class="math inline">\(p\)</span> is the fraction by which you’re scaling the search space (the fraction of the space you’re cramming all the probability mass into).</p></li>
<li><p><span class="math inline">\(\frac{1}{p}\)</span> is how many times smaller the search space is (how many times smaller the space containing all the probability mass is).</p></li>
<li><p>The <span class="math inline">\(N\)</span> in <span class="math inline">\(2^N=\frac{1}{p}\)</span> is then how many times you need to cut the search space in two for it to be <span class="math inline">\(\frac{1}{p}\)</span> times smaller (how many times you need to relocate half the probability mass to fit it all in a space <span class="math inline">\(\frac{1}{p}\)</span> times smaller).</p></li>
<li><p>So <span class="math inline">\(N=\log_2{\frac{1}{p}} =\)</span> the number of bits gained.</p></li>
</ol>
<h4 id="iii.">III.</h4>
<p>The expected value of a random variable <span class="math inline">\(X\)</span> with distribution <span class="math inline">\(P\)</span> is <span class="math display">\[E(X)=\sum_{i}{}x_i P(x_i)\]</span> If we treat the number of bits <span class="math inline">\(\log_2{\frac{1}{p}}\)</span> of information gained as a random variable, then we have a probability <span class="math inline">\(p\)</span> of gaining <span class="math inline">\(\log_2{\frac{1}{p}}\)</span> bits of information from any sample of this distribution.</p>
<p>This is what <em>information entropy</em> is: the expected amount of information gained from a single sample of a probability distribution (the expected number of times you need to relocate half of the probability mass to fit it all within the space of the sampled event): <span class="math display">\[H(X)=E(\log_2{\frac{1}{p}})=\sum p\log_2{\frac{1}{p}}=-\sum p_i\log_2{p}\]</span></p>
<p>Here are some distributions: <span class="math display">\[D_1=\{p_1=1,p_2=0\}\]</span> <span class="math display">\[D_2=\{p_1=0.5, p_2=0.5\}\]</span> <span class="math display">\[D_3=\{p_1=0.5, p_2=0.2, p_3=0.3\}\]</span></p>
<ul>
<li><p>The entropy of <span class="math inline">\(D_1\)</span> is <span class="math inline">\(0\)</span> since all the probability mass is already concentrated at a single point.</p></li>
<li><p>The entropy of <span class="math inline">\(D_2\)</span> is <span class="math inline">\(1\)</span> since you you’re gueranted to sample a <span class="math inline">\(p=0.5\)</span> event which needs to relocate half the probability mass once to concentrated it all at a single point.</p></li>
<li></li>
</ul>
<p>Repeating the mantra from earlier: the more information you gain the more probability mass gets concentrated in a smaller space. And the more probability mass gets concentrated in a smaller space the less information there is to gain, so the information entropy is smaller. In a way information gain is a property of an event while information entropy is a property of a distribution, in the same way probability is a property of an event while expected value is the property of a distribution.</p>
</body>
</html>
